{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blessed-scholarship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TODO:\n",
    "    transformers:\n",
    "        OneHotEncode columns\n",
    "            sex(DONE), cabin(DONE), embarked(DONE)\n",
    "        Figure out how to add these to a pipeline\n",
    "        extract numerics from ticket to get families traveling together(DONE)\n",
    "    END: create data processing pipeline\n",
    "DONE:\n",
    "\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "serious-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/c/titanic/overview\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "titanic_raw = pd.read_csv('./titanic_train.csv')\n",
    "print(titanic_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "superior-crest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncolumns to remove\\n    PassengerID\\n    Name\\n    Ticket\\n    ? Fare\\ncategorical\\n    sex\\n    ? Cabin\\n    Embarked (C = Cherbourg, Q = Queenstown, S  = Southampton)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_raw.head()\n",
    "\n",
    "'''\n",
    "columns to remove\n",
    "    PassengerID\n",
    "    Name\n",
    "    Ticket\n",
    "    ? Fare\n",
    "categorical\n",
    "    sex\n",
    "    ? Cabin\n",
    "    Embarked (C = Cherbourg, Q = Queenstown, S  = Southampton)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expensive-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a custom Transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Make OneHot encoding a column suck less\n",
    "def OneHotSetup(X, onehotcol, dropfirst=True, droporiginal=False):\n",
    "    # Create a new OneHotEncoder\n",
    "    if dropfirst:\n",
    "        dropfirst = 'first'\n",
    "        hdr_offset = 1\n",
    "    else:\n",
    "        dropfirst = None\n",
    "        hdr_offset = 0\n",
    "        \n",
    "    onehot = OneHotEncoder(sparse=False, drop=dropfirst) # remove 1st category\n",
    "    \n",
    "    # Fit our encoder\n",
    "    X_onehot = onehot.fit(X[onehotcol].values.reshape(-1,1)) # Fit onehot to cabin_letter\n",
    "    \n",
    "    # Get a list of column names, minus the first one\n",
    "    X_onehot_hdrs = X_onehot.categories_[0][hdr_offset:] # Take the list of categories minus the first one\n",
    "    \n",
    "    # Transform header to OG column name underscore category\n",
    "    temp = []\n",
    "    for val in X_onehot_hdrs:\n",
    "        val = onehotcol + '_' + val\n",
    "        temp.append(val)\n",
    "    X_onehot_hdrs = temp\n",
    "    \n",
    "    # Create a DataFrame of new OneHot columns\n",
    "    X_onehot_df = pd.DataFrame(onehot.transform(X[onehotcol].values.reshape(-1,1)))\n",
    "\n",
    "    # Redefine the headers of the onehot cols\n",
    "    X_onehot_df.columns = X_onehot_hdrs\n",
    "    \n",
    "    # Add new onehot columns to the original DF \n",
    "    X = pd.concat([X, X_onehot_df], axis=1)\n",
    "    \n",
    "    # Delete OneHot-ted column, if requested\n",
    "    if droporiginal:\n",
    "        X = X.drop([onehotcol], axis=1)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Custom Transform class\n",
    "class InitAttributeCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "            return self # Nothing to do here, apparently\n",
    "    def transform(self, X, y=None):\n",
    "        # Remove unnecessary columns\n",
    "        X = X.drop(['PassengerId', 'Name'],axis=1)\n",
    "        \n",
    "        # Handle the \"Cabin\" feature\n",
    "        #     1. split into features \"cabin_letter\" and \"cabin_number\"\n",
    "        #     2. replace nulls in both new columns with 'unreported'\n",
    "        #     3. Use OneHotSetup to add onehotcolumns\n",
    "        #     4. Replace NaN \"cabin_number\"s with 0, transform column to numeric\n",
    "          #cabin_letter\n",
    "        newcols = ['cabin_letter', 'cabin_number']                                 # New column names\n",
    "        X[newcols] = X['Cabin'].str.extract(r'(\\D*)(\\d*)') # Split column\n",
    "        X['cabin_letter'] = X[newcols].fillna('unreported')       # Replace nulls with 'unreported'\n",
    "        X['cabin_letter'] = X[newcols].replace('', 'unreported')  # Replace nulls with 'unreported'\n",
    "        X = OneHotSetup(X, 'cabin_letter', dropfirst=True, droporiginal=True)  # Call OneHotSetup\n",
    "        X = X.drop(['cabin_letter_unreported'], axis=1)    # Remove cabin_letter_unreported (useless)\n",
    "          #cabin_number\n",
    "        X['cabin_number'] = X['cabin_number'].fillna(value=0) # Replace NA with 0\n",
    "        X['cabin_number'] = X['cabin_number'].replace('', 0).astype('int64') # Replace '' with 0 and cast to int64\n",
    "        X = X.drop(['Cabin'], axis=1)\n",
    "\n",
    "        # Handle \"sex\" feature\n",
    "        X = OneHotSetup(X, 'Sex', dropfirst=True, droporiginal=True)\n",
    "\n",
    "        # Handle \"Embarked\" feature\n",
    "        X['Embarked'] = X['Embarked'].fillna('U')  # Fill missing values with U for Unknown\n",
    "        X = OneHotSetup(X, 'Embarked', dropfirst=False, droporiginal=False) # OneHotEncode it\n",
    "        X = X.drop(['Embarked'], axis=1)   # Remove original category\n",
    "        try:\n",
    "            X = X.drop(['Embarked_U'], axis=1) # Remove unknown category, if necessary\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle \"Age\" feature (we're going to impute a mean for missing values)\n",
    "        X['Age'] = X['Age'].fillna(X['Age'].mean())\n",
    "\n",
    "        # Handle \"Ticket\" column (remove characters, leave only numerics)\n",
    "        X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "        X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "        X = X.drop(['Ticket'], axis=1)\n",
    "        \n",
    "        self.final_columns = list(X.columns)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-fireplace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age' 'Fare' 'cabin_number' 'ticket_number' 'Survived' 'Pclass' 'SibSp'\n",
      " 'Parch' 'cabin_letter_B' 'cabin_letter_C' 'cabin_letter_D'\n",
      " 'cabin_letter_E' 'cabin_letter_F' 'cabin_letter_F E' 'cabin_letter_F G'\n",
      " 'cabin_letter_G' 'Sex_male' 'Embarked_C' 'Embarked_Q' 'Embarked_S']\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning (Pipeline conversion)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "standardize_columns = ['Age', 'Fare', 'cabin_number', 'ticket_number']\n",
    "\n",
    "columntransformer = ColumnTransformer([\n",
    "                        ('standardizer', StandardScaler(), standardize_columns)\n",
    "                ], remainder='passthrough',)\n",
    "\n",
    "pipe = Pipeline([\n",
    "                    ('init_clean', InitAttributeCleaner()),\n",
    "                    ('col_trans', columntransformer)\n",
    "                ])\n",
    "'''\n",
    "what a shitty to use API\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the\n",
    "transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed\n",
    "feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right\n",
    "to the output of the transformers.\n",
    "'''\n",
    "titanic_clean = pd.DataFrame(pipe.fit_transform(titanic_raw))\n",
    "\n",
    "# transformed columns have the passthrough columns added after, so need to come up with new column order and rename\n",
    "passthrough_columns = [x for x in pipe.named_steps.init_clean.final_columns if x not in standardize_columns]\n",
    "\n",
    "titanic_clean.columns = standardize_columns + passthrough_columns\n",
    "\n",
    "titanic_clean = titanic_clean.drop(['cabin_letter_T'], axis=1)\n",
    "\n",
    "print(titanic_clean.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "veterinary-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to split our test/train sets before going any further!\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Okay so first I need to split my X and y\n",
    "X = titanic_clean.drop(['Survived'], axis=1)\n",
    "y = titanic_clean['Survived']\n",
    "\n",
    "# And now split using stratified shuffle split\n",
    "splitter = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numeric-dominant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726890756302521\n",
      "0.7721518987341772\n",
      "0.42616033755274263\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Okay now I need to shortlist some models\n",
    "I have no idea how to do this\n",
    "I suppose this is a binary classifier soooo\n",
    "we'll start off with the SGD Classifier\n",
    "'''\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "#     print('train:', train_index, 'test', test_index)\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train.iloc[train_index]\n",
    "    y_train_folds = y_train.iloc[train_index]\n",
    "    X_test_fold = X_train.iloc[test_index]\n",
    "    y_test_fold = y_train.iloc[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "practical-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd_scores = cross_val_score(sgd_clf, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "flying-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EveryoneDies(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "everyonedies_clf = EveryoneDies()\n",
    "everysonedies_scores = cross_val_score(everyonedies_clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Okay cool so I'm doing better than just guessing noone made it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "annual-excerpt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Alright let's try a RandomForestClassifier as well\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "patient-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's score out a Guassian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_clf = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb_clf, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "framed-field",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try a logistic classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(multi_class='ovr')\n",
    "lr_scores = cross_val_score(lr_clf, X_train, y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "endless-porter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier avg: 0.7599724219442529)\n",
      "EveryoneDies avg: 0.6165468334482419)\n",
      "RandomForestClassifier avg: 0.8330203442879499)\n",
      "GaussianNB avg: 0.7219147050132966)\n",
      "SGD Classifier avg: 0.7599724219442529)\n",
      "LogisticRegression avg: 0.7977895148669797)\n"
     ]
    }
   ],
   "source": [
    "print(f'SGD Classifier avg: {sgd_scores.mean()})')\n",
    "print(f'EveryoneDies avg: {everysonedies_scores.mean()})')\n",
    "print(f'RandomForestClassifier avg: {forest_scores.mean()})')\n",
    "print(f'GaussianNB avg: {gnb_scores.mean()})')\n",
    "print(f'SGD Classifier avg: {sgd_scores.mean()})')\n",
    "print(f'LogisticRegression avg: {lr_scores.mean()})')\n",
    "\n",
    "# So it looks like RandomForestClassifier is the best looking model so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incorporate-farmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage correct = 63.687150837988824\n"
     ]
    }
   ],
   "source": [
    "# quick test on forest classifier on test set?\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "aa = pd.DataFrame(forest_clf.predict(X_test)).value_counts()\n",
    "\n",
    "correct = aa[0]\n",
    "incorrect= aa[1]\n",
    "print(f'percentage correct = {(correct/(incorrect+correct)*100)}')\n",
    "\n",
    "# weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continental-table",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8343346794051021\n"
     ]
    }
   ],
   "source": [
    "# Okay time to try tuning the forest\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model_params = {\"n_estimators\" : randint(1, 200),\n",
    "                \"max_features\" : truncnorm(a=0., b=1., loc=0.5, scale=0.01),\n",
    "                \"min_samples_split\" : uniform(0.01, 0.199)\n",
    "               }\n",
    "               \n",
    "\n",
    "rscv = RandomizedSearchCV(forest_clf, model_params, n_iter = 50, cv=5,\n",
    "                          n_jobs = 2, scoring=\"accuracy\")\n",
    "\n",
    "rscv.fit(X_train, y_train)\n",
    "\n",
    "print(rscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accomplished-worse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_forest_clf = rscv.best_estimator_\n",
    "\n",
    "# quick test on new forest classifier on test set\n",
    "new_forest_clf.score(X_test, y_test)\n",
    "\n",
    "test_results = new_forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "familiar-western",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Okay let's get a final score from the competition test set!\n",
    "titanic_test = pd.read_csv('./titanic_test.csv')\n",
    "titanic_test.info()\n",
    "\n",
    "# Store our Passenger IDs to use for our submission\n",
    "test_set_ids = titanic_test['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dying-fitting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "what a shitty to use API\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the\n",
    "transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed\n",
    "feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right\n",
    "to the output of the transformers.\n",
    "'''\n",
    "titanic_test_clean = pd.DataFrame(pipe.fit_transform(titanic_test))\n",
    "\n",
    "# transformed columns have the passthrough columns added after, so need to come up with new column order and rename\n",
    "passthrough_columns = [x for x in pipe.named_steps.init_clean.final_columns if x not in standardize_columns]\n",
    "\n",
    "titanic_test_clean.columns = standardize_columns + passthrough_columns\n",
    "\n",
    "titanic_test_clean['Fare'] = titanic_test_clean['Fare'].fillna(titanic_test_clean['Fare'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "widespread-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(new_forest_clf.predict(titanic_test_clean).astype(int))\n",
    "\n",
    "test_results = pd.concat([test_set_ids, test_results], axis=1)\n",
    "\n",
    "test_results.columns = ['PassengerId', 'survived']\n",
    "\n",
    "test_results.to_csv('titanic_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interesting-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "########## RE-CLEANING THE DATA FOR BETTER RESULTS! ##########\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "mechanical-detroit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "'''\n",
    "Alright let's come up with a plan for EVERY SINGLE FEATURE\n",
    "PassengerId\n",
    "    DROP\n",
    "Survived\n",
    "    split into new vector, then DROP\n",
    "Pclass\n",
    "    these are 1, 2, or 3\n",
    "        Should I -1 to get 0, 1, 2?\n",
    "        Should I standardize/normalize this?\n",
    "        Should I encode this column?\n",
    "Name\n",
    "    DROP\n",
    "Sex\n",
    "    male or female\n",
    "        *onehotencode this guy, DROP one category and original column\n",
    "Age\n",
    "    177 NULL values\n",
    "    from almost 0 to 80ish\n",
    "        *let's fill in the NULL values with median/mean\n",
    "        *then standardize/normalize\n",
    "SibSp (# of siblins/spouses aboard the titanic: sibling = [step]brother/[step]sister, spource = husband/wife)\n",
    "    I think I want to leave this one as categorical and not make it continous?\n",
    "    this is an ORDINAL catergorical feature\n",
    "        *Don't touch\n",
    "Parch (# of parents/children aboard: mother, father, [step]daughter, [step]son. Nannoy does NOT count)\n",
    "    this is another ORDINAL categorical feature\n",
    "        *Don't touch\n",
    "Ticket - 681 uniques, no nulls\n",
    "    I'm not sure how to handle this\n",
    "    Treat it categorically, but there are too many features for it to matter\n",
    "        *.1: DROP\n",
    "        *.2: Numericize, categorize(Onehot)\n",
    "        *.3: Numericize, categorize(featurehashing)\n",
    "Fare - Only 248 unique values wow (this has a very exponential shape hmm)\n",
    "    *leave as continous\n",
    "Cabin 687 nulls, 148 distinct values\n",
    "    *Extract Deck (A, B, etc)\n",
    "    *ordinally encode: A top deck, B below A, C below B, etc. Down to G\n",
    "Embarked\n",
    "    *onehotencode\n",
    "'''\n",
    "\n",
    "class InitAttributeCleaner2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ticket_handle, hash_number=10):\n",
    "        self.ticket_handle = ticket_handle\n",
    "        self.hash_number = hash_number\n",
    "    def fit(self, X, y=None):\n",
    "            return self # Nothing to do here, apparently\n",
    "    def transform(self, X, y=None):\n",
    "        #PassengerId: DROP\n",
    "        X = X.drop(['PassengerId'], axis=1)\n",
    "        \n",
    "        #Pclass: I don't want to categorize these, beacuse class 1 > 2 > 3 in SES (socioeconomic standing)\n",
    "        #    So we're going to recenter from 1, 2, 3 to 0, 1, 2\n",
    "        X['Pclass'] = X['Pclass'] - 1\n",
    "        \n",
    "        #Name: DROP\n",
    "        X = X.drop(['Name'], axis=1)\n",
    "        \n",
    "        #Sex: OneHotEncode this guy, then remove the original and female column\n",
    "        X = pd.concat([X, pd.get_dummies(X['Sex'])], axis=1)\n",
    "        X = X.drop(['Sex', 'female'], axis=1) # I could use \"drop_first\" param above for 'female' but w/e\n",
    "        \n",
    "        #Age: Fill NULLs with median, then standardize or normalize\n",
    "        X['Age'] = X['Age'].fillna(X['Age'].median())\n",
    "        \n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        X['Age'] = min_max_scaler.fit_transform(np.array(X['Age']).reshape(-1,1))\n",
    "        \n",
    "        \n",
    "        if(self.ticket_handle == 'drop'):\n",
    "            pass\n",
    "            \n",
    "        elif(self.ticket_handle == 'numeric'):\n",
    "            X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "            X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "            \n",
    "        elif(self.ticket_handle == 'onehot'):\n",
    "            X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "            X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "            X = pd.concat([X, pd.get_dummies(X['ticket_number'], drop_first=True, prefix='ticket_num')], axis=1)\n",
    "            X = X.drop(['ticket_number'], axis=1)\n",
    "            \n",
    "        elif(self.ticket_handle == 'hash'):\n",
    "            hasher = FeatureHasher(n_features=self.hash_number, input_type='string') # Create our hasher\n",
    "\n",
    "            hash_cols = [] # Populate a list so we can have column names\n",
    "            for i in range(0, self.hash_number):\n",
    "                hash_cols.append(f'ticket_hash_{i}')\n",
    "            # send it\n",
    "            hashed_cat = hasher.fit_transform(X['Ticket']).todense()\n",
    "            hashed_cat = pd.DataFrame(hashed_cat, columns=hash_cols)\n",
    "            \n",
    "            # This is necessary for god knows why\n",
    "            X.reset_index(drop=True, inplace=True)\n",
    "            hashed_cat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            X = pd.concat([X, hashed_cat], axis=1)\n",
    "            \n",
    "        else:\n",
    "            raise RuntimeError('invalid handler for Ticket. Try drop, numeric, onehot, or hash')\n",
    "\n",
    "        X = X.drop(['Ticket'], axis=1)\n",
    "        \n",
    "        \n",
    "        #Cabin: Extract Deck, ordinally encode\n",
    "        X['deck'] = X['Cabin'].str.extract(r'(\\D*)\\d*') # Split column\n",
    "        X['deck'] = X['deck'].fillna('U')\n",
    "\n",
    "        mapper = {'A' : 0,\n",
    "                  'B' : 1,\n",
    "                  'C' : 2,\n",
    "                  'D' : 3,\n",
    "                  'E' : 4,\n",
    "                  'F E' : 5,\n",
    "                  'F' : 6,\n",
    "                  'F G' : 7,\n",
    "                  'G' : 8,\n",
    "                  'T' : 9,\n",
    "                  'U' : 10\n",
    "                }\n",
    "        X['deck'] = X['deck'].replace(mapper)\n",
    "        X = X.drop(['Cabin'], axis=1)\n",
    "        \n",
    "        \n",
    "        #Embarked: OneHotEncode this bad boy\n",
    "        X = pd.concat([X, pd.get_dummies(X['Embarked'], prefix='embarked')], axis=1)\n",
    "        X = X.drop(['Embarked'], axis=1)\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "impressed-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like I've handled ever feature! Time to send it!\n",
    "X = titanic_raw.drop(['Survived'], axis=1)\n",
    "y = titanic_raw.loc[:, 'Survived']\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "experimental-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newcols = ['cabin_letter', 'cabin_number']                                 # New column names\n",
    "# X[newcols] = X['Cabin'].str.extract(r'(\\D*)(\\d*)') # Split column\n",
    "X = titanic_raw\n",
    "\n",
    "# X = X.drop(['Sex', 'female'], axis=1) # I could use \"drop_first\" param above for 'female' but w/e\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "sustainable-economy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleantype: drop\n",
      "tickethandle: drop, best score: 0.827351521717719, best params: {'max_features': 0.5518781082566893, 'min_samples_split': 0.025844306118618703, 'n_estimators': 118}\n",
      "\n",
      "cleantype: numeric\n",
      "tickethandle: numeric, best score: 0.8399783315276274, best params: {'max_features': 0.5111487598136749, 'min_samples_split': 0.0116647394336059, 'n_estimators': 223}\n",
      "\n",
      "cleantype: onehot\n",
      "tickethandle: onehot, best score: 0.8385797301290259, best params: {'max_features': 0.5174243960252174, 'min_samples_split': 0.011261021770732618, 'n_estimators': 291}\n",
      "\n",
      "cleantype: hash\n",
      "tickethandle: hash, best score: 0.8399290849995076, best params: {'max_features': 0.5369024826852938, 'min_samples_split': 0.010901775287247553, 'n_estimators': 327}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create the pipeline we're going to use with GridSearchCV\n",
    "# We'll create 1 X_train for each type of transformer ticket handles first\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "model_params = {'n_estimators' : randint(4, 400),\n",
    "                'max_features' : truncnorm(a=0., b=1., loc=0.5, scale=0.1),\n",
    "                'min_samples_split' : uniform(0.01, 0.5)\n",
    "               }\n",
    "\n",
    "ticket_clean_types = ['drop', 'numeric', 'onehot', 'hash']\n",
    "ticket_clean_best_estimators = dict((el,0) for el in ticket_clean_types)\n",
    "\n",
    "# Alright we want to find the most accurate estimator for the 4 different ways I came up with to handle \"Ticket\" feature\n",
    "for cleantype in ticket_clean_types:\n",
    "    print(f'cleantype: {cleantype}')\n",
    "    # Reset X_train\n",
    "    X = titanic_raw.drop(['Survived'], axis=1)\n",
    "    y = titanic_raw.loc[:, 'Survived']\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "    for train_index, test_index in splitter.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    rf_clf = RandomForestClassifier()\n",
    "    \n",
    "    # Clean X_train\n",
    "    cleaner = InitAttributeCleaner2(ticket_handle=cleantype, hash_number=10)\n",
    "\n",
    "    X_train = cleaner.fit_transform(X_train)\n",
    "\n",
    "    # Fit, print best params and score, store the best estimator\n",
    "    rscv = RandomizedSearchCV(rf_clf, model_params, n_iter=300, cv=5, n_jobs=4, scoring='accuracy')\n",
    "\n",
    "    rscv.fit(X_train, y_train)\n",
    "    \n",
    "    ticket_clean_best_estimators[cleantype] = rscv.best_estimator_\n",
    "    print(f'tickethandle: {cleantype}, best score: {rscv.best_score_}, best params: {rscv.best_params_}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to go with the Random Forest since it did best last time\n",
    "# We're going to use the GridSearchCV:\n",
    "#    Not randomized because I want to check out the differnent hyperparameters in the transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
