{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "blessed-scholarship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TODO:\n",
    "    transformers:\n",
    "        OneHotEncode columns\n",
    "            sex(DONE), cabin(DONE), embarked(DONE)\n",
    "        Figure out how to add these to a pipeline\n",
    "        extract numerics from ticket to get families traveling together(DONE)\n",
    "    END: create data processing pipeline\n",
    "DONE:\n",
    "\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "serious-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/c/titanic/overview\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "titanic_raw = pd.read_csv('./titanic_train.csv')\n",
    "print(titanic_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "superior-crest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncolumns to remove\\n    PassengerID\\n    Name\\n    Ticket\\n    ? Fare\\ncategorical\\n    sex\\n    ? Cabin\\n    Embarked (C = Cherbourg, Q = Queenstown, S  = Southampton)\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_raw.head()\n",
    "\n",
    "'''\n",
    "columns to remove\n",
    "    PassengerID\n",
    "    Name\n",
    "    Ticket\n",
    "    ? Fare\n",
    "categorical\n",
    "    sex\n",
    "    ? Cabin\n",
    "    Embarked (C = Cherbourg, Q = Queenstown, S  = Southampton)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "expensive-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a custom Transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Make OneHot encoding a column suck less\n",
    "def OneHotSetup(X, onehotcol, dropfirst=True, droporiginal=False):\n",
    "    # Create a new OneHotEncoder\n",
    "    if dropfirst:\n",
    "        dropfirst = 'first'\n",
    "        hdr_offset = 1\n",
    "    else:\n",
    "        dropfirst = None\n",
    "        hdr_offset = 0\n",
    "        \n",
    "    onehot = OneHotEncoder(sparse=False, drop=dropfirst) # remove 1st category\n",
    "    \n",
    "    # Fit our encoder\n",
    "    X_onehot = onehot.fit(X[onehotcol].values.reshape(-1,1)) # Fit onehot to cabin_letter\n",
    "    \n",
    "    # Get a list of column names, minus the first one\n",
    "    X_onehot_hdrs = X_onehot.categories_[0][hdr_offset:] # Take the list of categories minus the first one\n",
    "    \n",
    "    # Transform header to OG column name underscore category\n",
    "    temp = []\n",
    "    for val in X_onehot_hdrs:\n",
    "        val = onehotcol + '_' + val\n",
    "        temp.append(val)\n",
    "    X_onehot_hdrs = temp\n",
    "    \n",
    "    # Create a DataFrame of new OneHot columns\n",
    "    X_onehot_df = pd.DataFrame(onehot.transform(X[onehotcol].values.reshape(-1,1)))\n",
    "\n",
    "    # Redefine the headers of the onehot cols\n",
    "    X_onehot_df.columns = X_onehot_hdrs\n",
    "    \n",
    "    # Add new onehot columns to the original DF \n",
    "    X = pd.concat([X, X_onehot_df], axis=1)\n",
    "    \n",
    "    # Delete OneHot-ted column, if requested\n",
    "    if droporiginal:\n",
    "        X = X.drop([onehotcol], axis=1)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Custom Transform class\n",
    "class InitAttributeCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "            return self # Nothing to do here, apparently\n",
    "    def transform(self, X, y=None):\n",
    "        # Remove unnecessary columns\n",
    "        X = X.drop(['PassengerId', 'Name'],axis=1)\n",
    "        \n",
    "        # Handle the \"Cabin\" feature\n",
    "        #     1. split into features \"cabin_letter\" and \"cabin_number\"\n",
    "        #     2. replace nulls in both new columns with 'unreported'\n",
    "        #     3. Use OneHotSetup to add onehotcolumns\n",
    "        #     4. Replace NaN \"cabin_number\"s with 0, transform column to numeric\n",
    "          #cabin_letter\n",
    "        newcols = ['cabin_letter', 'cabin_number']                                 # New column names\n",
    "        X[newcols] = X['Cabin'].str.extract(r'(\\D*)(\\d*)') # Split column\n",
    "        X['cabin_letter'] = X[newcols].fillna('unreported')       # Replace nulls with 'unreported'\n",
    "        X['cabin_letter'] = X[newcols].replace('', 'unreported')  # Replace nulls with 'unreported'\n",
    "        X = OneHotSetup(X, 'cabin_letter', dropfirst=True, droporiginal=True)  # Call OneHotSetup\n",
    "        X = X.drop(['cabin_letter_unreported'], axis=1)    # Remove cabin_letter_unreported (useless)\n",
    "          #cabin_number\n",
    "        X['cabin_number'] = X['cabin_number'].fillna(value=0) # Replace NA with 0\n",
    "        X['cabin_number'] = X['cabin_number'].replace('', 0).astype('int64') # Replace '' with 0 and cast to int64\n",
    "        X = X.drop(['Cabin'], axis=1)\n",
    "\n",
    "        # Handle \"sex\" feature\n",
    "        X = OneHotSetup(X, 'Sex', dropfirst=True, droporiginal=True)\n",
    "\n",
    "        # Handle \"Embarked\" feature\n",
    "        X['Embarked'] = X['Embarked'].fillna('U')  # Fill missing values with U for Unknown\n",
    "        X = OneHotSetup(X, 'Embarked', dropfirst=False, droporiginal=False) # OneHotEncode it\n",
    "        X = X.drop(['Embarked'], axis=1)   # Remove original category\n",
    "        try:\n",
    "            X = X.drop(['Embarked_U'], axis=1) # Remove unknown category, if necessary\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle \"Age\" feature (we're going to impute a mean for missing values)\n",
    "        X['Age'] = X['Age'].fillna(X['Age'].mean())\n",
    "\n",
    "        # Handle \"Ticket\" column (remove characters, leave only numerics)\n",
    "        X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "        X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "        X = X.drop(['Ticket'], axis=1)\n",
    "        \n",
    "        self.final_columns = list(X.columns)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "illegal-fireplace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age' 'Fare' 'cabin_number' 'ticket_number' 'Survived' 'Pclass' 'SibSp'\n",
      " 'Parch' 'cabin_letter_B' 'cabin_letter_C' 'cabin_letter_D'\n",
      " 'cabin_letter_E' 'cabin_letter_F' 'cabin_letter_F E' 'cabin_letter_F G'\n",
      " 'cabin_letter_G' 'Sex_male' 'Embarked_C' 'Embarked_Q' 'Embarked_S']\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning (Pipeline conversion)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "standardize_columns = ['Age', 'Fare', 'cabin_number', 'ticket_number']\n",
    "\n",
    "columntransformer = ColumnTransformer([\n",
    "                        ('standardizer', StandardScaler(), standardize_columns)\n",
    "                ], remainder='passthrough',)\n",
    "\n",
    "pipe = Pipeline([\n",
    "                    ('init_clean', InitAttributeCleaner()),\n",
    "                    ('col_trans', columntransformer)\n",
    "                ])\n",
    "'''\n",
    "what a shitty to use API\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the\n",
    "transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed\n",
    "feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right\n",
    "to the output of the transformers.\n",
    "'''\n",
    "titanic_clean = pd.DataFrame(pipe.fit_transform(titanic_raw))\n",
    "\n",
    "# transformed columns have the passthrough columns added after, so need to come up with new column order and rename\n",
    "passthrough_columns = [x for x in pipe.named_steps.init_clean.final_columns if x not in standardize_columns]\n",
    "\n",
    "titanic_clean.columns = standardize_columns + passthrough_columns\n",
    "\n",
    "titanic_clean = titanic_clean.drop(['cabin_letter_T'], axis=1)\n",
    "\n",
    "print(titanic_clean.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "veterinary-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to split our test/train sets before going any further!\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Okay so first I need to split my X and y\n",
    "X = titanic_clean.drop(['Survived'], axis=1)\n",
    "y = titanic_clean['Survived']\n",
    "\n",
    "# And now split using stratified shuffle split\n",
    "splitter = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "numeric-dominant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726890756302521\n",
      "0.7721518987341772\n",
      "0.42616033755274263\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Okay now I need to shortlist some models\n",
    "I have no idea how to do this\n",
    "I suppose this is a binary classifier soooo\n",
    "we'll start off with the SGD Classifier\n",
    "'''\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "#     print('train:', train_index, 'test', test_index)\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train.iloc[train_index]\n",
    "    y_train_folds = y_train.iloc[train_index]\n",
    "    X_test_fold = X_train.iloc[test_index]\n",
    "    y_test_fold = y_train.iloc[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "practical-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd_scores = cross_val_score(sgd_clf, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "flying-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EveryoneDies(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "everyonedies_clf = EveryoneDies()\n",
    "everysonedies_scores = cross_val_score(everyonedies_clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Okay cool so I'm doing better than just guessing noone made it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "annual-excerpt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Alright let's try a RandomForestClassifier as well\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "patient-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's score out a Guassian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_clf = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb_clf, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "framed-field",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try a logistic classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(multi_class='ovr')\n",
    "lr_scores = cross_val_score(lr_clf, X_train, y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "endless-porter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier avg: 0.7599724219442529)\n",
      "EveryoneDies avg: 0.6165468334482419)\n",
      "RandomForestClassifier avg: 0.8330203442879499)\n",
      "GaussianNB avg: 0.7219147050132966)\n",
      "SGD Classifier avg: 0.7599724219442529)\n",
      "LogisticRegression avg: 0.7977895148669797)\n"
     ]
    }
   ],
   "source": [
    "print(f'SGD Classifier avg: {sgd_scores.mean()})')\n",
    "print(f'EveryoneDies avg: {everysonedies_scores.mean()})')\n",
    "print(f'RandomForestClassifier avg: {forest_scores.mean()})')\n",
    "print(f'GaussianNB avg: {gnb_scores.mean()})')\n",
    "print(f'SGD Classifier avg: {sgd_scores.mean()})')\n",
    "print(f'LogisticRegression avg: {lr_scores.mean()})')\n",
    "\n",
    "# So it looks like RandomForestClassifier is the best looking model so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "incorporate-farmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage correct = 63.687150837988824\n"
     ]
    }
   ],
   "source": [
    "# quick test on forest classifier on test set?\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "aa = pd.DataFrame(forest_clf.predict(X_test)).value_counts()\n",
    "\n",
    "correct = aa[0]\n",
    "incorrect= aa[1]\n",
    "print(f'percentage correct = {(correct/(incorrect+correct)*100)}')\n",
    "\n",
    "# weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "continental-table",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.838569880823402\n"
     ]
    }
   ],
   "source": [
    "# Okay time to try tuning the forest\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model_params = {\"n_estimators\" : randint(1, 200),\n",
    "                \"max_features\" : truncnorm(a=0., b=1., loc=0.5, scale=0.01),\n",
    "                \"min_samples_split\" : uniform(0.01, 0.199)\n",
    "               }\n",
    "               \n",
    "\n",
    "rscv = RandomizedSearchCV(forest_clf, model_params, n_iter = 50, cv=5,\n",
    "                          n_jobs = 2, scoring=\"accuracy\")\n",
    "\n",
    "rscv.fit(X_train, y_train)\n",
    "\n",
    "print(rscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "accomplished-worse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_forest_clf = rscv.best_estimator_\n",
    "\n",
    "# quick test on new forest classifier on test set\n",
    "new_forest_clf.score(X_test, y_test)\n",
    "\n",
    "test_results = new_forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "familiar-western",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Okay let's get a final score from the competition test set!\n",
    "titanic_test = pd.read_csv('./titanic_test.csv')\n",
    "titanic_test.info()\n",
    "\n",
    "# Store our Passenger IDs to use for our submission\n",
    "test_set_ids = titanic_test['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dying-fitting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "what a shitty to use API\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the\n",
    "transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed\n",
    "feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right\n",
    "to the output of the transformers.\n",
    "'''\n",
    "titanic_test_clean = pd.DataFrame(pipe.fit_transform(titanic_test))\n",
    "\n",
    "# transformed columns have the passthrough columns added after, so need to come up with new column order and rename\n",
    "passthrough_columns = [x for x in pipe.named_steps.init_clean.final_columns if x not in standardize_columns]\n",
    "\n",
    "titanic_test_clean.columns = standardize_columns + passthrough_columns\n",
    "\n",
    "titanic_test_clean['Fare'] = titanic_test_clean['Fare'].fillna(titanic_test_clean['Fare'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "widespread-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(new_forest_clf.predict(titanic_test_clean).astype(int))\n",
    "\n",
    "test_results = pd.concat([test_set_ids, test_results], axis=1)\n",
    "\n",
    "test_results.columns = ['PassengerId', 'survived']\n",
    "\n",
    "test_results.to_csv('titanic_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "lightweight-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "########## RE-CLEANING THE DATA FOR BETTER RESULTS! ##########\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "material-homeless",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "'''\n",
    "Alright let's come up with a plan for EVERY SINGLE FEATURE\n",
    "PassengerId\n",
    "    DROP\n",
    "Survived\n",
    "    split into new vector, then DROP\n",
    "Pclass\n",
    "    these are 1, 2, or 3\n",
    "        Should I -1 to get 0, 1, 2?\n",
    "        Should I standardize/normalize this?\n",
    "        Should I encode this column?\n",
    "Name\n",
    "    DROP\n",
    "Sex\n",
    "    male or female\n",
    "        *onehotencode this guy, DROP one category and original column\n",
    "Age\n",
    "    177 NULL values\n",
    "    from almost 0 to 80ish\n",
    "        *let's fill in the NULL values with median/mean\n",
    "        *then standardize/normalize\n",
    "SibSp (# of siblins/spouses aboard the titanic: sibling = [step]brother/[step]sister, spource = husband/wife)\n",
    "    I think I want to leave this one as categorical and not make it continous?\n",
    "    this is an ORDINAL catergorical feature\n",
    "        *Don't touch\n",
    "Parch (# of parents/children aboard: mother, father, [step]daughter, [step]son. Nannoy does NOT count)\n",
    "    this is another ORDINAL categorical feature\n",
    "        *Don't touch\n",
    "Ticket - 681 uniques, no nulls\n",
    "    I'm not sure how to handle this\n",
    "    Treat it categorically, but there are too many features for it to matter\n",
    "        *.1: DROP\n",
    "        *.2: Numericize, categorize(Onehot)\n",
    "        *.3: Numericize, categorize(featurehashing)\n",
    "Fare - Only 248 unique values wow (this has a very exponential shape hmm)\n",
    "    *leave as continous\n",
    "Cabin 687 nulls, 148 distinct values\n",
    "    *Extract Deck (A, B, etc)\n",
    "    *ordinally encode: A top deck, B below A, C below B, etc. Down to G\n",
    "Embarked\n",
    "    *onehotencode\n",
    "'''\n",
    "\n",
    "class InitAttributeCleaner2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ticket_handle, hash_number=10):\n",
    "        self.ticket_handle = ticket_handle\n",
    "        self.hash_number = hash_number\n",
    "    def fit(self, X, y=None):\n",
    "            return self # Nothing to do here, apparently\n",
    "    def transform(self, X, y=None):\n",
    "        #PassengerId: DROP\n",
    "        X = X.drop(['PassengerId'], axis=1)\n",
    "        \n",
    "        #Pclass: I don't want to categorize these, beacuse class 1 > 2 > 3 in SES (socioeconomic standing)\n",
    "        #    So we're going to recenter from 1, 2, 3 to 0, 1, 2\n",
    "        X['Pclass'] = X['Pclass'] - 1\n",
    "        \n",
    "        #Name: DROP\n",
    "        X = X.drop(['Name'], axis=1)\n",
    "        \n",
    "        #Sex: OneHotEncode this guy, then remove the original and female column\n",
    "        X = pd.concat([X, pd.get_dummies(X['Sex'])], axis=1)\n",
    "        X = X.drop(['Sex', 'female'], axis=1) # I could use \"drop_first\" param above for 'female' but w/e\n",
    "        \n",
    "        #Age: Fill NULLs with median, then standardize or normalize\n",
    "        X['Age'] = X['Age'].fillna(X['Age'].median())\n",
    "        \n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        X['Age'] = min_max_scaler.fit_transform(np.array(X['Age']).reshape(-1,1))\n",
    "        \n",
    "        \n",
    "        if(self.ticket_handle == 'drop'):\n",
    "            pass\n",
    "            \n",
    "        elif(self.ticket_handle == 'numeric'):\n",
    "            X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "            X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "            \n",
    "        elif(self.ticket_handle == 'onehot'):\n",
    "            X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "            X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "            X = pd.concat([X, pd.get_dummies(X['ticket_number'], drop_first=True, prefix='ticket_num')], axis=1)\n",
    "            X = X.drop(['ticket_number'], axis=1)\n",
    "            \n",
    "        elif(self.ticket_handle == 'hash'):\n",
    "            hasher = FeatureHasher(n_features=self.hash_number, input_type='string') # Create our hasher\n",
    "\n",
    "            hash_cols = [] # Populate a list so we can have column names\n",
    "            for i in range(0, self.hash_number):\n",
    "                hash_cols.append(f'ticket_hash_{i}')\n",
    "            # send it\n",
    "            X = pd.concat([X, pd.DataFrame.sparse.from_spmatrix(fh.fit_transform(X['Ticket']), columns=hash_cols)], axis=1)\n",
    "            \n",
    "        else:\n",
    "            raise RuntimeError('invalid handler for Ticket. Try drop, numeric, onehot, or hash')\n",
    "\n",
    "        X = X.drop(['Ticket'], axis=1)\n",
    "        \n",
    "        \n",
    "        #Cabin: Extract Deck, ordinally encode\n",
    "        X['deck'] = X['Cabin'].str.extract(r'(\\D*)\\d*') # Split column\n",
    "        X['deck'] = X['deck'].fillna('U')\n",
    "\n",
    "        mapper = {'A' : 0,\n",
    "                  'B' : 1,\n",
    "                  'C' : 2,\n",
    "                  'D' : 3,\n",
    "                  'E' : 4,\n",
    "                  'F E' : 5,\n",
    "                  'F' : 6,\n",
    "                  'F G' : 7,\n",
    "                  'G' : 8,\n",
    "                  'T' : 9,\n",
    "                  'U' : 10\n",
    "                }\n",
    "        X['deck'] = X['deck'].replace(mapper)\n",
    "        X = X.drop(['Cabin'], axis=1)\n",
    "        \n",
    "        \n",
    "        #Embarked: OneHotEncode this bad boy\n",
    "        X = pd.concat([X, pd.get_dummies(X['Embarked'], prefix='embarked')], axis=1)\n",
    "        X = X.drop(['Embarked'], axis=1)\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "chief-rugby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>ticket_number</th>\n",
       "      <th>deck</th>\n",
       "      <th>male</th>\n",
       "      <th>ticket_hash_0</th>\n",
       "      <th>...</th>\n",
       "      <th>ticket_hash_3</th>\n",
       "      <th>ticket_hash_4</th>\n",
       "      <th>ticket_hash_5</th>\n",
       "      <th>ticket_hash_6</th>\n",
       "      <th>ticket_hash_7</th>\n",
       "      <th>ticket_hash_8</th>\n",
       "      <th>ticket_hash_9</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>21171</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>17599</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>3101282</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>113803</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>373450</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass       Age  SibSp  Parch     Fare  ticket_number  deck  \\\n",
       "0         0       2  0.271174      1      0   7.2500          21171    10   \n",
       "1         1       0  0.472229      1      0  71.2833          17599     2   \n",
       "2         1       2  0.321438      0      0   7.9250        3101282    10   \n",
       "3         1       0  0.434531      1      0  53.1000         113803     2   \n",
       "4         0       2  0.434531      0      0   8.0500         373450    10   \n",
       "\n",
       "   male  ticket_hash_0  ...  ticket_hash_3  ticket_hash_4  ticket_hash_5  \\\n",
       "0     1            1.0  ...            0.0            0.0            0.0   \n",
       "1     0            1.0  ...            0.0            0.0            0.0   \n",
       "2     0            1.0  ...           -2.0            0.0            0.0   \n",
       "3     0            2.0  ...           -1.0            0.0            0.0   \n",
       "4     1            3.0  ...            0.0            0.0            0.0   \n",
       "\n",
       "   ticket_hash_6  ticket_hash_7  ticket_hash_8  ticket_hash_9  embarked_C  \\\n",
       "0            0.0           -3.0            1.0            1.0           0   \n",
       "1            0.0           -1.0            1.0            0.0           1   \n",
       "2            0.0           -3.0           -1.0            0.0           0   \n",
       "3            0.0           -2.0            0.0           -1.0           0   \n",
       "4            0.0            0.0            1.0           -1.0           0   \n",
       "\n",
       "   embarked_Q  embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEMP Class tester cell\n",
    "trns = InitAttributeCleaner2(ticket_handle='hash', hash_number=10)\n",
    "ghg = trns.transform(titanic_raw)\n",
    "\n",
    "ghg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "overhead-clock",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>ticket_number</th>\n",
       "      <th>deck</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>21171</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>17599</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>3101282</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>113803</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>373450</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>0.334004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>211536</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>0.233476</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "      <td>112053</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>0.346569</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>6607</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "      <td>111369</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>0.396833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>370376</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex       Age  \\\n",
       "0                              Braund, Mr. Owen Harris    male  0.271174   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  0.472229   \n",
       "2                               Heikkinen, Miss. Laina  female  0.321438   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  0.434531   \n",
       "4                             Allen, Mr. William Henry    male  0.434531   \n",
       "..                                                 ...     ...       ...   \n",
       "886                              Montvila, Rev. Juozas    male  0.334004   \n",
       "887                       Graham, Miss. Margaret Edith  female  0.233476   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female  0.346569   \n",
       "889                              Behr, Mr. Karl Howell    male  0.321438   \n",
       "890                                Dooley, Mr. Patrick    male  0.396833   \n",
       "\n",
       "     SibSp  Parch            Ticket     Fare Cabin Embarked  ticket_number  \\\n",
       "0        1      0         A/5 21171   7.2500   NaN        S          21171   \n",
       "1        1      0          PC 17599  71.2833   C85        C          17599   \n",
       "2        0      0  STON/O2. 3101282   7.9250   NaN        S        3101282   \n",
       "3        1      0            113803  53.1000  C123        S         113803   \n",
       "4        0      0            373450   8.0500   NaN        S         373450   \n",
       "..     ...    ...               ...      ...   ...      ...            ...   \n",
       "886      0      0            211536  13.0000   NaN        S         211536   \n",
       "887      0      0            112053  30.0000   B42        S         112053   \n",
       "888      1      2        W./C. 6607  23.4500   NaN        S           6607   \n",
       "889      0      0            111369  30.0000  C148        C         111369   \n",
       "890      0      0            370376   7.7500   NaN        Q         370376   \n",
       "\n",
       "     deck  embarked_C  embarked_Q  embarked_S  \n",
       "0      10           0           0           1  \n",
       "1       2           1           0           0  \n",
       "2      10           0           0           1  \n",
       "3       2           0           0           1  \n",
       "4      10           0           0           1  \n",
       "..    ...         ...         ...         ...  \n",
       "886    10           0           0           1  \n",
       "887     1           0           0           1  \n",
       "888    10           0           0           1  \n",
       "889     2           1           0           0  \n",
       "890    10           0           1           0  \n",
       "\n",
       "[891 rows x 17 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# newcols = ['cabin_letter', 'cabin_number']                                 # New column names\n",
    "# X[newcols] = X['Cabin'].str.extract(r'(\\D*)(\\d*)') # Split column\n",
    "X = titanic_raw\n",
    "\n",
    "\n",
    "X\n",
    "# X = X.drop(['Sex', 'female'], axis=1) # I could use \"drop_first\" param above for 'female' but w/e\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
