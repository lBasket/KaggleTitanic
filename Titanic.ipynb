{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blessed-scholarship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TODO:\n",
    "    transformers:\n",
    "        OneHotEncode columns\n",
    "            sex(DONE), cabin(DONE), embarked(DONE)\n",
    "        Figure out how to add these to a pipeline\n",
    "        extract numerics from ticket to get families traveling together(DONE)\n",
    "    END: create data processing pipeline\n",
    "DONE:\n",
    "\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "serious-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/c/titanic/overview\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "titanic_raw = pd.read_csv('./titanic_train.csv')\n",
    "print(titanic_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "superior-crest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncolumns to remove\\n    PassengerID\\n    Name\\n    Ticket\\n    ? Fare\\ncategorical\\n    sex\\n    ? Cabin\\n    Embarked (C = Cherbourg, Q = Queenstown, S  = Southampton)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_raw.head()\n",
    "\n",
    "'''\n",
    "columns to remove\n",
    "    PassengerID\n",
    "    Name\n",
    "    Ticket\n",
    "    ? Fare\n",
    "categorical\n",
    "    sex\n",
    "    ? Cabin\n",
    "    Embarked (C = Cherbourg, Q = Queenstown, S  = Southampton)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expensive-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a custom Transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Make OneHot encoding a column suck less\n",
    "def OneHotSetup(X, onehotcol, dropfirst=True, droporiginal=False):\n",
    "    # Create a new OneHotEncoder\n",
    "    if dropfirst:\n",
    "        dropfirst = 'first'\n",
    "        hdr_offset = 1\n",
    "    else:\n",
    "        dropfirst = None\n",
    "        hdr_offset = 0\n",
    "        \n",
    "    onehot = OneHotEncoder(sparse=False, drop=dropfirst) # remove 1st category\n",
    "    \n",
    "    # Fit our encoder\n",
    "    X_onehot = onehot.fit(X[onehotcol].values.reshape(-1,1)) # Fit onehot to cabin_letter\n",
    "    \n",
    "    # Get a list of column names, minus the first one\n",
    "    X_onehot_hdrs = X_onehot.categories_[0][hdr_offset:] # Take the list of categories minus the first one\n",
    "    \n",
    "    # Transform header to OG column name underscore category\n",
    "    temp = []\n",
    "    for val in X_onehot_hdrs:\n",
    "        val = onehotcol + '_' + val\n",
    "        temp.append(val)\n",
    "    X_onehot_hdrs = temp\n",
    "    \n",
    "    # Create a DataFrame of new OneHot columns\n",
    "    X_onehot_df = pd.DataFrame(onehot.transform(X[onehotcol].values.reshape(-1,1)))\n",
    "\n",
    "    # Redefine the headers of the onehot cols\n",
    "    X_onehot_df.columns = X_onehot_hdrs\n",
    "    \n",
    "    # Add new onehot columns to the original DF \n",
    "    X = pd.concat([X, X_onehot_df], axis=1)\n",
    "    \n",
    "    # Delete OneHot-ted column, if requested\n",
    "    if droporiginal:\n",
    "        X = X.drop([onehotcol], axis=1)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Custom Transform class\n",
    "class InitAttributeCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "            return self # Nothing to do here, apparently\n",
    "    def transform(self, X, y=None):\n",
    "        # Remove unnecessary columns\n",
    "        X = X.drop(['PassengerId', 'Name'],axis=1)\n",
    "        \n",
    "        # Handle the \"Cabin\" feature\n",
    "        #     1. split into features \"cabin_letter\" and \"cabin_number\"\n",
    "        #     2. replace nulls in both new columns with 'unreported'\n",
    "        #     3. Use OneHotSetup to add onehotcolumns\n",
    "        #     4. Replace NaN \"cabin_number\"s with 0, transform column to numeric\n",
    "          #cabin_letter\n",
    "        newcols = ['cabin_letter', 'cabin_number']                                 # New column names\n",
    "        X[newcols] = X['Cabin'].str.extract(r'(\\D*)(\\d*)') # Split column\n",
    "        X['cabin_letter'] = X[newcols].fillna('unreported')       # Replace nulls with 'unreported'\n",
    "        X['cabin_letter'] = X[newcols].replace('', 'unreported')  # Replace nulls with 'unreported'\n",
    "        X = OneHotSetup(X, 'cabin_letter', dropfirst=True, droporiginal=True)  # Call OneHotSetup\n",
    "        X = X.drop(['cabin_letter_unreported'], axis=1)    # Remove cabin_letter_unreported (useless)\n",
    "          #cabin_number\n",
    "        X['cabin_number'] = X['cabin_number'].fillna(value=0) # Replace NA with 0\n",
    "        X['cabin_number'] = X['cabin_number'].replace('', 0).astype('int64') # Replace '' with 0 and cast to int64\n",
    "        X = X.drop(['Cabin'], axis=1)\n",
    "\n",
    "        # Handle \"sex\" feature\n",
    "        X = OneHotSetup(X, 'Sex', dropfirst=True, droporiginal=True)\n",
    "\n",
    "        # Handle \"Embarked\" feature\n",
    "        X['Embarked'] = X['Embarked'].fillna('U')  # Fill missing values with U for Unknown\n",
    "        X = OneHotSetup(X, 'Embarked', dropfirst=False, droporiginal=False) # OneHotEncode it\n",
    "        X = X.drop(['Embarked', 'Embarked_U'], axis=1)   # Remove Unknown category, original category\n",
    "\n",
    "        # Handle \"Age\" feature (we're going to impute a mean for missing values)\n",
    "        X['Age'] = X['Age'].fillna(X['Age'].mean())\n",
    "\n",
    "        # Handle \"Ticket\" column (remove characters, leave only numerics)\n",
    "        X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "        X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "        X = X.drop(['Ticket'], axis=1)\n",
    "        \n",
    "        self.final_columns = list(X.columns)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-fireplace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age      Fare  cabin_number  ticket_number  Survived  Pclass  SibSp  \\\n",
      "0 -0.592481 -0.502445     -0.421322      -0.420410       0.0     3.0    1.0   \n",
      "1  0.638789  0.786845      2.738590      -0.425854       1.0     1.0    1.0   \n",
      "2 -0.284663 -0.488854     -0.421322       4.274382       1.0     3.0    0.0   \n",
      "3  0.407926  0.420730      4.151256      -0.279217       1.0     1.0    1.0   \n",
      "4  0.407926 -0.486337     -0.421322       0.116544       0.0     3.0    0.0   \n",
      "\n",
      "   Parch  cabin_letter_B  cabin_letter_C  ...  cabin_letter_E  cabin_letter_F  \\\n",
      "0    0.0             0.0             0.0  ...             0.0             0.0   \n",
      "1    0.0             0.0             1.0  ...             0.0             0.0   \n",
      "2    0.0             0.0             0.0  ...             0.0             0.0   \n",
      "3    0.0             0.0             1.0  ...             0.0             0.0   \n",
      "4    0.0             0.0             0.0  ...             0.0             0.0   \n",
      "\n",
      "   cabin_letter_F E  cabin_letter_F G  cabin_letter_G  cabin_letter_T  \\\n",
      "0               0.0               0.0             0.0             0.0   \n",
      "1               0.0               0.0             0.0             0.0   \n",
      "2               0.0               0.0             0.0             0.0   \n",
      "3               0.0               0.0             0.0             0.0   \n",
      "4               0.0               0.0             0.0             0.0   \n",
      "\n",
      "   Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
      "0       1.0         0.0         0.0         1.0  \n",
      "1       0.0         1.0         0.0         0.0  \n",
      "2       0.0         0.0         0.0         1.0  \n",
      "3       0.0         0.0         0.0         1.0  \n",
      "4       1.0         0.0         0.0         1.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning (Pipeline conversion)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "standardize_columns = ['Age', 'Fare', 'cabin_number', 'ticket_number']\n",
    "\n",
    "columntransformer = ColumnTransformer([\n",
    "                        ('standardizer', StandardScaler(), standardize_columns)\n",
    "                ], remainder='passthrough',)\n",
    "\n",
    "pipe = Pipeline([\n",
    "                    ('init_clean', InitAttributeCleaner()),\n",
    "                    ('col_trans', columntransformer)\n",
    "                ])\n",
    "'''\n",
    "what a shitty to use API\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the\n",
    "transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed\n",
    "feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right\n",
    "to the output of the transformers.\n",
    "'''\n",
    "titanic_clean = pd.DataFrame(pipe.fit_transform(titanic_raw))\n",
    "\n",
    "# transformed columns have the passthrough columns added after, so need to come up with new column order and rename\n",
    "passthrough_columns = [x for x in pipe.named_steps.init_clean.final_columns if x not in standardize_columns]\n",
    "\n",
    "titanic_clean.columns = standardize_columns + passthrough_columns\n",
    "\n",
    "print(titanic_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "veterinary-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to split our test/train sets before going any further!\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Okay so first I need to split my X and y\n",
    "X = titanic_clean.drop(['Survived'], axis=1)\n",
    "y = titanic_clean['Survived']\n",
    "\n",
    "# And now split using stratified shuffle split\n",
    "splitter = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numeric-dominant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7689075630252101\n",
      "0.7721518987341772\n",
      "0.7468354430379747\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Okay now I need to shortlist some models\n",
    "I have no idea how to do this\n",
    "I suppose this is a binary classifier soooo\n",
    "we'll start off with the SGD Classifier\n",
    "'''\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "#     print('train:', train_index, 'test', test_index)\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train.iloc[train_index]\n",
    "    y_train_folds = y_train.iloc[train_index]\n",
    "    X_test_fold = X_train.iloc[test_index]\n",
    "    y_test_fold = y_train.iloc[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "practical-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd_scores = cross_val_score(sgd_clf, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "flying-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EveryoneDies(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "everyonedies_clf = EveryoneDies()\n",
    "everysonedies_scores = cross_val_score(everyonedies_clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Okay cool so I'm doing better than just guessing noone made it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "annual-excerpt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Alright let's try a RandomForestClassifier as well\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "patient-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's score out a Guassian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_clf = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb_clf, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "framed-field",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try a logistic classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(multi_class='ovr')\n",
    "lr_scores = cross_val_score(lr_clf, X_train, y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fatal-affiliation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier avg: 0.7416921107061952)\n",
      "EveryoneDies avg: 0.6165468334482419)\n",
      "RandomForestClassifier avg: 0.8273474178403756)\n",
      "GaussianNB avg: 0.7037427361371023)\n",
      "SGD Classifier avg: 0.7416921107061952)\n",
      "LogisticRegression avg: 0.7977895148669797)\n"
     ]
    }
   ],
   "source": [
    "print(f'SGD Classifier avg: {sgd_scores.mean()})')\n",
    "print(f'EveryoneDies avg: {everysonedies_scores.mean()})')\n",
    "print(f'RandomForestClassifier avg: {forest_scores.mean()})')\n",
    "print(f'GaussianNB avg: {gnb_scores.mean()})')\n",
    "print(f'SGD Classifier avg: {sgd_scores.mean()})')\n",
    "print(f'LogisticRegression avg: {lr_scores.mean()})')\n",
    "\n",
    "# So it looks like RandomForestClassifier is the best looking model so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "pursuant-transcription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage correct = 62.56983240223464\n"
     ]
    }
   ],
   "source": [
    "# quick test on forest classifier on test set?\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "aa = pd.DataFrame(forest_clf.predict(X_test)).value_counts()\n",
    "\n",
    "correct = aa[0]\n",
    "incorrect= aa[1]\n",
    "print(f'percentage correct = {(correct/(incorrect+correct)*100)}')\n",
    "\n",
    "# weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "moving-knitting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8343740766275978\n"
     ]
    }
   ],
   "source": [
    "# Okay time to try tuning the forest\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model_params = {\"n_estimators\" : randint(1, 200),\n",
    "                \"max_features\" : truncnorm(a=0., b=1., loc=0.5, scale=0.01),\n",
    "                \"min_samples_split\" : uniform(0.01, 0.199)\n",
    "               }\n",
    "               \n",
    "\n",
    "rscv = RandomizedSearchCV(forest_clf, model_params, n_iter = 50, cv=5,\n",
    "                          n_jobs = 2, scoring=\"accuracy\")\n",
    "\n",
    "rscv.fit(X_train, y_train)\n",
    "\n",
    "print(rscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "brown-petroleum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage correct = 66.4804469273743\n"
     ]
    }
   ],
   "source": [
    "new_forest_clf = rscv.best_estimator_\n",
    "\n",
    "# quick test on new forest classifier on test set\n",
    "aa = pd.DataFrame(new_forest_clf.predict(X_test)).value_counts()\n",
    "\n",
    "correct = aa[0]\n",
    "incorrect= aa[1]\n",
    "print(f'percentage correct = {(correct/(incorrect+correct)*100)}')\n",
    "\n",
    "# weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "herbal-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.838550182212154\n"
     ]
    }
   ],
   "source": [
    "# What if i trried the LR model\n",
    "\n",
    "lr_clf = LogisticRegression(multi_class='ovr')\n",
    "\n",
    "model_params = {\"n_estimators\" : randint(1, 200),\n",
    "                \"max_features\" : truncnorm(a=0., b=1., loc=0.5, scale=0.01),\n",
    "                \"min_samples_split\" : uniform(0.01, 0.199)\n",
    "               }\n",
    "               \n",
    "\n",
    "rscv_lr = RandomizedSearchCV(forest_clf, model_params, n_iter = 50, cv=5,\n",
    "                          n_jobs = 2, scoring=\"accuracy\")\n",
    "\n",
    "rscv_lr.fit(X_train, y_train)\n",
    "\n",
    "print(rscv_lr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "needed-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage correct = 66.4804469273743\n"
     ]
    }
   ],
   "source": [
    "new_lr_clf = rscv_lr.best_estimator_\n",
    "\n",
    "# quick test on new forest classifier on test set\n",
    "aa = pd.DataFrame(new_lr_clf.predict(X_test)).value_counts()\n",
    "\n",
    "correct = aa[0]\n",
    "incorrect= aa[1]\n",
    "print(f'percentage correct = {(correct/(incorrect+correct)*100)}')\n",
    "\n",
    "# weak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
