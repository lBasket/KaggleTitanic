{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blessed-scholarship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TODO:\n",
    "    transformers:\n",
    "        OneHotEncode columns\n",
    "            sex(DONE), cabin(DONE), embarked(DONE)\n",
    "        Figure out how to add these to a pipeline\n",
    "        extract numerics from ticket to get families traveling together(DONE)\n",
    "    END: create data processing pipeline\n",
    "DONE:\n",
    "\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "serious-makeup",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/c/titanic/overview\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "titanic_raw = pd.read_csv('./titanic_train.csv')\n",
    "print(titanic_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "superior-crest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncolumns to remove\\n    PassengerID\\n    Name\\n    Ticket\\n    ? Fare\\ncategorical\\n    sex\\n    ? Cabin\\n    Embarked (C = Cherbourg, Q = Queenstown, S  = Southampton)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_raw.head()\n",
    "\n",
    "'''\n",
    "columns to remove\n",
    "    PassengerID\n",
    "    Name\n",
    "    Ticket\n",
    "    ? Fare\n",
    "categorical\n",
    "    sex\n",
    "    ? Cabin\n",
    "    Embarked (C = Cherbourg, Q = Queenstown, S  = Southampton)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "expensive-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a custom Transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Make OneHot encoding a column suck less\n",
    "def OneHotSetup(X, onehotcol, dropfirst=True, droporiginal=False):\n",
    "    # Create a new OneHotEncoder\n",
    "    if dropfirst:\n",
    "        dropfirst = 'first'\n",
    "        hdr_offset = 1\n",
    "    else:\n",
    "        dropfirst = None\n",
    "        hdr_offset = 0\n",
    "        \n",
    "    onehot = OneHotEncoder(sparse=False, drop=dropfirst) # remove 1st category\n",
    "    \n",
    "    # Fit our encoder\n",
    "    X_onehot = onehot.fit(X[onehotcol].values.reshape(-1,1)) # Fit onehot to cabin_letter\n",
    "    \n",
    "    # Get a list of column names, minus the first one\n",
    "    X_onehot_hdrs = X_onehot.categories_[0][hdr_offset:] # Take the list of categories minus the first one\n",
    "    \n",
    "    # Transform header to OG column name underscore category\n",
    "    temp = []\n",
    "    for val in X_onehot_hdrs:\n",
    "        val = onehotcol + '_' + val\n",
    "        temp.append(val)\n",
    "    X_onehot_hdrs = temp\n",
    "    \n",
    "    # Create a DataFrame of new OneHot columns\n",
    "    X_onehot_df = pd.DataFrame(onehot.transform(X[onehotcol].values.reshape(-1,1)))\n",
    "\n",
    "    # Redefine the headers of the onehot cols\n",
    "    X_onehot_df.columns = X_onehot_hdrs\n",
    "    \n",
    "    # Add new onehot columns to the original DF \n",
    "    X = pd.concat([X, X_onehot_df], axis=1)\n",
    "    \n",
    "    # Delete OneHot-ted column, if requested\n",
    "    if droporiginal:\n",
    "        X = X.drop([onehotcol], axis=1)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Custom Transform class\n",
    "class InitAttributeCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "            return self # Nothing to do here, apparently\n",
    "    def transform(self, X, y=None):\n",
    "        # Remove unnecessary columns\n",
    "        X = X.drop(['PassengerId', 'Name'],axis=1)\n",
    "        \n",
    "        # Handle the \"Cabin\" feature\n",
    "        #     1. split into features \"cabin_letter\" and \"cabin_number\"\n",
    "        #     2. replace nulls in both new columns with 'unreported'\n",
    "        #     3. Use OneHotSetup to add onehotcolumns\n",
    "        #     4. Replace NaN \"cabin_number\"s with 0, transform column to numeric\n",
    "          #cabin_letter\n",
    "#         newcols = ['cabin_letter', 'cabin_number']                                 # New column names\n",
    "#         X[newcols] = X['Cabin'].str.extract(r'(\\D*)(\\d*)') # Split column\n",
    "#         X['cabin_letter'] = X[newcols].fillna('unreported')       # Replace nulls with 'unreported'\n",
    "#         X['cabin_letter'] = X[newcols].replace('', 'unreported')  # Replace nulls with 'unreported'\n",
    "#         X = OneHotSetup(X, 'cabin_letter', dropfirst=True, droporiginal=True)  # Call OneHotSetup\n",
    "#         X = X.drop(['cabin_letter_unreported'], axis=1)    # Remove cabin_letter_unreported (useless)\n",
    "#           #cabin_number\n",
    "#         X['cabin_number'] = X['cabin_number'].fillna(value=0) # Replace NA with 0\n",
    "#         X['cabin_number'] = X['cabin_number'].replace('', 0).astype('int64') # Replace '' with 0 and cast to int64\n",
    "        X = X.drop(['Cabin'], axis=1)\n",
    "\n",
    "        # Handle \"sex\" feature\n",
    "        X = OneHotSetup(X, 'Sex', dropfirst=True, droporiginal=True)\n",
    "\n",
    "        # Handle \"Embarked\" feature\n",
    "        X['Embarked'] = X['Embarked'].fillna('U')  # Fill missing values with U for Unknown\n",
    "        X = OneHotSetup(X, 'Embarked', dropfirst=False, droporiginal=False) # OneHotEncode it\n",
    "        X = X.drop(['Embarked'], axis=1)   # Remove original category\n",
    "        try:\n",
    "            X = X.drop(['Embarked_U'], axis=1) # Remove unknown category, if necessary\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle \"Age\" feature (we're going to impute a mean for missing values)\n",
    "        X['Age'] = X['Age'].fillna(X['Age'].mean())\n",
    "\n",
    "        # Handle \"Ticket\" column (remove characters, leave only numerics)\n",
    "        X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "        X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "        X = X.drop(['Ticket'], axis=1)\n",
    "        \n",
    "        self.final_columns = list(X.columns)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-fireplace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age' 'Fare' 'cabin_number' 'ticket_number' 'Survived' 'Pclass' 'SibSp'\n",
      " 'Parch' 'cabin_letter_B' 'cabin_letter_C' 'cabin_letter_D'\n",
      " 'cabin_letter_E' 'cabin_letter_F' 'cabin_letter_F E' 'cabin_letter_F G'\n",
      " 'cabin_letter_G' 'Sex_male' 'Embarked_C' 'Embarked_Q' 'Embarked_S']\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning (Pipeline conversion)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "standardize_columns = ['Age', 'Fare', 'cabin_number', 'ticket_number']\n",
    "\n",
    "columntransformer = ColumnTransformer([\n",
    "                        ('standardizer', StandardScaler(), standardize_columns)\n",
    "                ], remainder='passthrough',)\n",
    "\n",
    "pipe = Pipeline([\n",
    "                    ('init_clean', InitAttributeCleaner()),\n",
    "                    ('col_trans', columntransformer)\n",
    "                ])\n",
    "'''\n",
    "what a shitty to use API\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the\n",
    "transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed\n",
    "feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right\n",
    "to the output of the transformers.\n",
    "'''\n",
    "titanic_clean = pd.DataFrame(pipe.fit_transform(titanic_raw))\n",
    "\n",
    "# transformed columns have the passthrough columns added after, so need to come up with new column order and rename\n",
    "passthrough_columns = [x for x in pipe.named_steps.init_clean.final_columns if x not in standardize_columns]\n",
    "\n",
    "titanic_clean.columns = standardize_columns + passthrough_columns\n",
    "\n",
    "titanic_clean = titanic_clean.drop(['cabin_letter_T'], axis=1)\n",
    "\n",
    "print(titanic_clean.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "veterinary-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to split our test/train sets before going any further!\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Okay so first I need to split my X and y\n",
    "X = titanic_clean.drop(['Survived'], axis=1)\n",
    "y = titanic_clean['Survived']\n",
    "\n",
    "# And now split using stratified shuffle split\n",
    "splitter = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numeric-dominant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726890756302521\n",
      "0.7721518987341772\n",
      "0.42616033755274263\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Okay now I need to shortlist some models\n",
    "I have no idea how to do this\n",
    "I suppose this is a binary classifier soooo\n",
    "we'll start off with the SGD Classifier\n",
    "'''\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "#     print('train:', train_index, 'test', test_index)\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train.iloc[train_index]\n",
    "    y_train_folds = y_train.iloc[train_index]\n",
    "    X_test_fold = X_train.iloc[test_index]\n",
    "    y_test_fold = y_train.iloc[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "practical-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd_scores = cross_val_score(sgd_clf, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "flying-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EveryoneDies(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "everyonedies_clf = EveryoneDies()\n",
    "everysonedies_scores = cross_val_score(everyonedies_clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Okay cool so I'm doing better than just guessing noone made it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "annual-excerpt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Alright let's try a RandomForestClassifier as well\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "patient-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's score out a Guassian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_clf = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb_clf, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "framed-field",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try a logistic classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(multi_class='ovr')\n",
    "lr_scores = cross_val_score(lr_clf, X_train, y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "endless-porter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier avg: 0.7599724219442529)\n",
      "EveryoneDies avg: 0.6165468334482419)\n",
      "RandomForestClassifier avg: 0.8330203442879499)\n",
      "GaussianNB avg: 0.7219147050132966)\n",
      "SGD Classifier avg: 0.7599724219442529)\n",
      "LogisticRegression avg: 0.7977895148669797)\n"
     ]
    }
   ],
   "source": [
    "print(f'SGD Classifier avg: {sgd_scores.mean()})')\n",
    "print(f'EveryoneDies avg: {everysonedies_scores.mean()})')\n",
    "print(f'RandomForestClassifier avg: {forest_scores.mean()})')\n",
    "print(f'GaussianNB avg: {gnb_scores.mean()})')\n",
    "print(f'SGD Classifier avg: {sgd_scores.mean()})')\n",
    "print(f'LogisticRegression avg: {lr_scores.mean()})')\n",
    "\n",
    "# So it looks like RandomForestClassifier is the best looking model so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incorporate-farmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage correct = 63.687150837988824\n"
     ]
    }
   ],
   "source": [
    "# quick test on forest classifier on test set?\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "aa = pd.DataFrame(forest_clf.predict(X_test)).value_counts()\n",
    "\n",
    "correct = aa[0]\n",
    "incorrect= aa[1]\n",
    "print(f'percentage correct = {(correct/(incorrect+correct)*100)}')\n",
    "\n",
    "# weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continental-table",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8329459273121245\n"
     ]
    }
   ],
   "source": [
    "# Okay time to try tuning the forest\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model_params = {\"n_estimators\" : randint(1, 200),\n",
    "                \"max_features\" : truncnorm(a=0., b=1., loc=0.5, scale=0.01),\n",
    "                \"min_samples_split\" : uniform(0.01, 0.199)\n",
    "               }\n",
    "               \n",
    "\n",
    "rscv = RandomizedSearchCV(forest_clf, model_params, n_iter = 50, cv=5,\n",
    "                          n_jobs = 2, scoring=\"accuracy\")\n",
    "\n",
    "rscv.fit(X_train, y_train)\n",
    "\n",
    "print(rscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accomplished-worse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_forest_clf = rscv.best_estimator_\n",
    "\n",
    "# quick test on new forest classifier on test set\n",
    "new_forest_clf.score(X_test, y_test)\n",
    "\n",
    "test_results = new_forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "familiar-western",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Okay let's get a final score from the competition test set!\n",
    "titanic_test = pd.read_csv('./titanic_test.csv')\n",
    "titanic_test.info()\n",
    "\n",
    "# Store our Passenger IDs to use for our submission\n",
    "test_set_ids = titanic_test['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dying-fitting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "what a shitty to use API\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the\n",
    "transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed\n",
    "feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right\n",
    "to the output of the transformers.\n",
    "'''\n",
    "titanic_test_clean = pd.DataFrame(pipe.fit_transform(titanic_test))\n",
    "\n",
    "# transformed columns have the passthrough columns added after, so need to come up with new column order and rename\n",
    "passthrough_columns = [x for x in pipe.named_steps.init_clean.final_columns if x not in standardize_columns]\n",
    "\n",
    "titanic_test_clean.columns = standardize_columns + passthrough_columns\n",
    "\n",
    "titanic_test_clean['Fare'] = titanic_test_clean['Fare'].fillna(titanic_test_clean['Fare'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "widespread-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(new_forest_clf.predict(titanic_test_clean).astype(int))\n",
    "\n",
    "test_results = pd.concat([test_set_ids, test_results], axis=1)\n",
    "\n",
    "test_results.columns = ['PassengerId', 'survived']\n",
    "\n",
    "test_results.to_csv('titanic_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interesting-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "########## RE-CLEANING THE DATA FOR BETTER RESULTS! ##########\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "mechanical-detroit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 130)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m130\u001b[0m\n\u001b[1;33m    }\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "'''\n",
    "Alright let's come up with a plan for EVERY SINGLE FEATURE\n",
    "PassengerId\n",
    "    DROP\n",
    "Survived\n",
    "    split into new vector, then DROP\n",
    "Pclass\n",
    "    these are 1, 2, or 3\n",
    "        Should I -1 to get 0, 1, 2?\n",
    "        Should I standardize/normalize this?\n",
    "        Should I encode this column?\n",
    "Name\n",
    "    DROP\n",
    "Sex\n",
    "    male or female\n",
    "        *onehotencode this guy, DROP one category and original column\n",
    "Age\n",
    "    177 NULL values\n",
    "    from almost 0 to 80ish\n",
    "        *let's fill in the NULL values with median/mean\n",
    "        *then standardize/normalize\n",
    "SibSp (# of siblins/spouses aboard the titanic: sibling = [step]brother/[step]sister, spource = husband/wife)\n",
    "    I think I want to leave this one as categorical and not make it continous?\n",
    "    this is an ORDINAL catergorical feature\n",
    "        *Don't touch\n",
    "Parch (# of parents/children aboard: mother, father, [step]daughter, [step]son. Nannoy does NOT count)\n",
    "    this is another ORDINAL categorical feature\n",
    "        *Don't touch\n",
    "Ticket - 681 uniques, no nulls\n",
    "    I'm not sure how to handle this\n",
    "    Treat it categorically, but there are too many features for it to matter\n",
    "        *.1: DROP\n",
    "        *.2: Numericize, categorize(Onehot)\n",
    "        *.3: Numericize, categorize(featurehashing)\n",
    "Fare - Only 248 unique values wow (this has a very exponential shape hmm)\n",
    "    *leave as continous\n",
    "    fillna with median, bc test set has a missing value\n",
    "Cabin 687 nulls, 148 distinct values\n",
    "    *Extract Deck (A, B, etc)\n",
    "    *ordinally encode: A top deck, B below A, C below B, etc. Down to G\n",
    "Embarked\n",
    "    *onehotencode\n",
    "'''\n",
    "\n",
    "class InitAttributeCleaner2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ticket_handle, hash_number=10):\n",
    "        self.ticket_handle = ticket_handle\n",
    "        self.hash_number = hash_number\n",
    "    def fit(self, X, y=None):\n",
    "            return self # Nothing to do here, apparently\n",
    "    def transform(self, X, y=None):\n",
    "        #PassengerId: DROP\n",
    "        X = X.drop(['PassengerId'], axis=1)\n",
    "        \n",
    "        #Pclass: I don't want to categorize these, beacuse class 1 > 2 > 3 in SES (socioeconomic standing)\n",
    "        #    So we're going to recenter from 1, 2, 3 to 0, 1, 2\n",
    "        X['Pclass'] = X['Pclass'] - 1\n",
    "        \n",
    "        #Name: DROP\n",
    "        X = X.drop(['Name'], axis=1)\n",
    "        \n",
    "        #Sex: OneHotEncode this guy, then remove the original and female column\n",
    "        X = pd.concat([X, pd.get_dummies(X['Sex'])], axis=1)\n",
    "        X = X.drop(['Sex', 'female'], axis=1) # I could use \"drop_first\" param above for 'female' but w/e\n",
    "        \n",
    "        #Age: Fill NULLs with median, then standardize or normalize\n",
    "        X = X.dropna(subset=['Age'])\n",
    "        #BIN THESE, equal samples per bin NOT equal \"size\" per bin\n",
    "        X['age_bins'] = pd.qcut(X['Age']), q=10, duplicates='drop', labels=False).astype('int')\n",
    "        X = X.drop(['Age'], axis=1)\n",
    "        \n",
    "#         min_max_scaler = MinMaxScaler()\n",
    "#         X['Age'] = min_max_scaler.fit_transform(np.array(X['Age']).reshape(-1,1))\n",
    "        # Let's bin this vad boi: quantile bins, or just specify them yourself\n",
    "        # https://pbpython.com/pandas-qcut-cut.html\n",
    "        \n",
    "        \n",
    "        if(self.ticket_handle == 'drop'):\n",
    "            pass\n",
    "            \n",
    "        elif(self.ticket_handle == 'numeric'):\n",
    "            X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "            X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "            \n",
    "        elif(self.ticket_handle == 'onehot'):\n",
    "            X['ticket_number'] = X['Ticket'].str.extract(r'(?:.*? ){0,2}(\\d*)') # Extract just the numeric at the end\n",
    "            X['ticket_number'] = X['ticket_number'].replace('', 0).astype('int64') # These few \"LINE\" values that are missed by RE\n",
    "            X = pd.concat([X, pd.get_dummies(X['ticket_number'], drop_first=True, prefix='ticket_num')], axis=1)\n",
    "            X = X.drop(['ticket_number'], axis=1)\n",
    "            \n",
    "        elif(self.ticket_handle == 'hash'):\n",
    "            hasher = FeatureHasher(n_features=self.hash_number, input_type='string') # Create our hasher\n",
    "\n",
    "            hash_cols = [] # Populate a list so we can have column names\n",
    "            for i in range(0, self.hash_number):\n",
    "                hash_cols.append(f'ticket_hash_{i}')\n",
    "            # send it\n",
    "            hashed_cat = hasher.fit_transform(X['Ticket']).todense()\n",
    "            hashed_cat = pd.DataFrame(hashed_cat, columns=hash_cols)\n",
    "            \n",
    "            # This is necessary for god knows why\n",
    "            X.reset_index(drop=True, inplace=True)\n",
    "            hashed_cat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            X = pd.concat([X, hashed_cat], axis=1)\n",
    "            \n",
    "        else:\n",
    "            raise RuntimeError('invalid handler for Ticket. Try drop, numeric, onehot, or hash')\n",
    "\n",
    "        X = X.drop(['Ticket'], axis=1)\n",
    "        \n",
    "        \n",
    "        #Cabin: Extract Deck, ordinally encode\n",
    "        X['deck'] = X['Cabin'].str.extract(r'(\\D*)\\d*') # Split column\n",
    "        X['deck'] = X['deck'].fillna('U')\n",
    "\n",
    "        mapper = {'A' : 0,\n",
    "                  'B' : 1,\n",
    "                  'C' : 2,\n",
    "                  'D' : 3,\n",
    "                  'E' : 4,\n",
    "                  'F E' : 5,\n",
    "                  'F' : 6,\n",
    "                  'F G' : 7,\n",
    "                  'G' : 8,\n",
    "                  'T' : 9,\n",
    "                  'U' : 10\n",
    "                }\n",
    "        X['deck'] = X['deck'].replace(mapper)\n",
    "        X = X.drop(['Cabin'], axis=1)\n",
    "        \n",
    "        \n",
    "        #Embarked: OneHotEncode this bad boy\n",
    "        X = pd.concat([X, pd.get_dummies(X['Embarked'], prefix='embarked')], axis=1)\n",
    "        X = X.drop(['Embarked'], axis=1)\n",
    "        \n",
    "        #Fare: fillna with median \n",
    "        X['Fare'] = X['Fare'].fillna(X['Fare'].median())\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "impressed-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like I've handled ever feature! Time to send it!\n",
    "X = titanic_raw.drop(['Survived'], axis=1)\n",
    "y = titanic_raw.loc[:, 'Survived']\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "experimental-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newcols = ['cabin_letter', 'cabin_number']                                 # New column names\n",
    "# X[newcols] = X['Cabin'].str.extract(r'(\\D*)(\\d*)') # Split column\n",
    "X = titanic_raw\n",
    "\n",
    "# X = X.drop(['Sex', 'female'], axis=1) # I could use \"drop_first\" param above for 'female' but w/e\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sustainable-economy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleantype: drop\n",
      "tickethandle: drop, best score: 0.8287599724219442, best params: {'max_features': 0.5561386063736451, 'min_samples_split': 0.022662279399163543, 'n_estimators': 449}\n",
      "\n",
      "cleantype: numeric\n",
      "tickethandle: numeric, best score: 0.83858957943465, best params: {'max_features': 0.5499837865901587, 'min_samples_split': 0.013046080812340044, 'n_estimators': 737}\n",
      "\n",
      "cleantype: hash\n",
      "tickethandle: hash, best score: 0.8399586329163793, best params: {'max_features': 0.5713526373410327, 'min_samples_split': 0.004023455490809179, 'n_estimators': 380}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create the pipeline we're going to use with GridSearchCV\n",
    "# We'll create 1 X_train for each type of transformer ticket handles first\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "model_params = {'n_estimators' : randint(300, 800),\n",
    "                'max_features' : truncnorm(a=0., b=1., loc=0.5, scale=0.1),\n",
    "                'min_samples_split' : uniform(0.001, 0.5)\n",
    "               }\n",
    "\n",
    "ticket_clean_types = ['drop', 'numeric', 'hash'] # We are removing onehot because it takes too long and have decided on hash\n",
    "ticket_clean_best_estimators = dict((el,0) for el in ticket_clean_types)\n",
    "\n",
    "# Alright we want to find the most accurate estimator for the 4 different ways I came up with to handle \"Ticket\" feature\n",
    "for cleantype in ticket_clean_types:\n",
    "    print(f'cleantype: {cleantype}')\n",
    "    # Reset X_train\n",
    "    X = titanic_raw.drop(['Survived'], axis=1)\n",
    "    y = titanic_raw.loc[:, 'Survived']\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "    for train_index, test_index in splitter.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    rf_clf = RandomForestClassifier()\n",
    "    \n",
    "    # Clean X_train\n",
    "    cleaner = InitAttributeCleaner2(ticket_handle=cleantype, hash_number=10)\n",
    "\n",
    "    X_train = cleaner.fit_transform(X_train)\n",
    "\n",
    "    # Fit, print best params and score, store the best estimator\n",
    "    rscv = RandomizedSearchCV(rf_clf, model_params, n_iter=100, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "    rscv.fit(X_train, y_train)\n",
    "    \n",
    "    ticket_clean_best_estimators[cleantype] = rscv.best_estimator_\n",
    "    print(f'tickethandle: {cleantype}, best score: {rscv.best_score_}, best params: {rscv.best_params_}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acoustic-linux",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel_params2 = {'n_estimators' : randint(400, 800),\\n                'max_features' : truncnorm(a=0.4, b=0.7, loc=0.5, scale=0.1),\\n                'min_samples_split' : uniform(0.001, 0.1)\\n               }\\nticket_clean_types = [3, 5, 7, 9, 11, 13, 30, 50, 100, 200, 500]\\nhashnum_best_estimators = dict((el,0) for el in ticket_clean_types)\\n\\nfor hashnumloop in ticket_clean_types:\\n    print(f'hashnum: {hashnumloop}')\\n    # Reset X_train\\n    X = titanic_raw.drop(['Survived'], axis=1)\\n    y = titanic_raw.loc[:, 'Survived']\\n\\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\\n\\n    for train_index, test_index in splitter.split(X, y):\\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\\n    \\n    rf_clf = RandomForestClassifier()\\n    \\n    # Clean X_train\\n    cleaner = InitAttributeCleaner2(ticket_handle='hash', hash_number=hashnumloop)\\n    \\n    X_train = cleaner.fit_transform(X_train)\\n\\n    # Fit, print best params and score, store the best estimator\\n    rscv = RandomizedSearchCV(rf_clf, model_params2, n_iter=200, cv=3, n_jobs=-1, scoring='accuracy')\\n\\n    rscv.fit(X_train, y_train)\\n    \\n    hashnum_best_estimators[hashnumloop] = rscv.best_estimator_\\n    print(f'hashnum: {hashnumloop}, best score: {rscv.best_score_}, best params: {rscv.best_params_}')\\n    print('')\\n\\n# Testing paramets, commented out bc testing no longer needed and runs very slow\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're going to go with the Random Forest since it did best last time\n",
    "# Looks like the \"hash \" type is doing the best no matter what... so now it is time to try different # of hash columns\n",
    "\n",
    "'''\n",
    "model_params2 = {'n_estimators' : randint(400, 800),\n",
    "                'max_features' : truncnorm(a=0.4, b=0.7, loc=0.5, scale=0.1),\n",
    "                'min_samples_split' : uniform(0.001, 0.1)\n",
    "               }\n",
    "ticket_clean_types = [3, 5, 7, 9, 11, 13, 30, 50, 100, 200, 500]\n",
    "hashnum_best_estimators = dict((el,0) for el in ticket_clean_types)\n",
    "\n",
    "for hashnumloop in ticket_clean_types:\n",
    "    print(f'hashnum: {hashnumloop}')\n",
    "    # Reset X_train\n",
    "    X = titanic_raw.drop(['Survived'], axis=1)\n",
    "    y = titanic_raw.loc[:, 'Survived']\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "    for train_index, test_index in splitter.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    rf_clf = RandomForestClassifier()\n",
    "    \n",
    "    # Clean X_train\n",
    "    cleaner = InitAttributeCleaner2(ticket_handle='hash', hash_number=hashnumloop)\n",
    "    \n",
    "    X_train = cleaner.fit_transform(X_train)\n",
    "\n",
    "    # Fit, print best params and score, store the best estimator\n",
    "    rscv = RandomizedSearchCV(rf_clf, model_params2, n_iter=200, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "    rscv.fit(X_train, y_train)\n",
    "    \n",
    "    hashnum_best_estimators[hashnumloop] = rscv.best_estimator_\n",
    "    print(f'hashnum: {hashnumloop}, best score: {rscv.best_score_}, best params: {rscv.best_params_}')\n",
    "    print('')\n",
    "\n",
    "# Testing paramets, commented out bc testing no longer needed and runs very slow\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "scientific-quebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features=0.5629948810999708,\n",
       "                       min_samples_split=0.00593040590826378, n_estimators=469)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our model with the parameters we have decided on\n",
    "\n",
    "# Reset X_train\n",
    "X = titanic_raw.drop(['Survived'], axis=1)\n",
    "y = titanic_raw.loc[:, 'Survived']\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "new_forest_clf2 = RandomForestClassifier(max_features=0.5629948810999708, min_samples_split=0.00593040590826378, n_estimators=469)\n",
    "\n",
    "# Clean X_train\n",
    "cleaner = InitAttributeCleaner2(ticket_handle='hash', hash_number=50)\n",
    "\n",
    "X_train = cleaner.fit_transform(X_train)\n",
    "\n",
    "# Fit, print best params and score, store the best estimator\n",
    "new_forest_clf2.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "treated-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best est: RandomForestClassifier(max_features=0.5570118585032531,\n",
      "                       min_samples_split=0.019493235284347432,\n",
      "                       n_estimators=744)\n",
      "best score: 0.8314541006275927, best params: {'max_features': 0.5570118585032531, 'min_samples_split': 0.019493235284347432, 'n_estimators': 744}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_params2 = {'n_estimators' : randint(400, 800),\n",
    "                'max_features' : truncnorm(a=0.4, b=0.7, loc=0.5, scale=0.1),\n",
    "                'min_samples_split' : uniform(0.001, 0.1)\n",
    "               }\n",
    "ticket_clean_types = [3, 5, 7, 9, 11, 13, 30, 50, 100, 200, 500]\n",
    "hashnum_best_estimators = dict((el,0) for el in ticket_clean_types)\n",
    "\n",
    "\n",
    "\n",
    "# Reset X_train\n",
    "X = titanic_raw.drop(['Survived'], axis=1)\n",
    "y = titanic_raw.loc[:, 'Survived']\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Clean X_train\n",
    "cleaner = InitAttributeCleaner2(ticket_handle='hash', hash_number=50)\n",
    "\n",
    "X_train = cleaner.fit_transform(X_train)\n",
    "\n",
    "# Fit, print best params and score, store the best estimator\n",
    "rscv = RandomizedSearchCV(rf_clf, model_params2, n_iter=300, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "rscv.fit(X_train, y_train)\n",
    "\n",
    "print(f'best est: {rscv.best_estimator_}')\n",
    "print(f'best score: {rscv.best_score_}, best params: {rscv.best_params_}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "homeless-average",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>age_bins</th>\n",
       "      <th>ticket_hash_0</th>\n",
       "      <th>ticket_hash_1</th>\n",
       "      <th>ticket_hash_2</th>\n",
       "      <th>ticket_hash_3</th>\n",
       "      <th>...</th>\n",
       "      <th>ticket_hash_44</th>\n",
       "      <th>ticket_hash_45</th>\n",
       "      <th>ticket_hash_46</th>\n",
       "      <th>ticket_hash_47</th>\n",
       "      <th>ticket_hash_48</th>\n",
       "      <th>ticket_hash_49</th>\n",
       "      <th>deck</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>221.7792</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.3500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  SibSp  Parch      Fare  male  age_bins  ticket_hash_0  \\\n",
       "0       2      0      0   56.4958     1         4            0.0   \n",
       "1       1      0      0    0.0000     1         4            1.0   \n",
       "2       0      0      0  221.7792     1         4            1.0   \n",
       "3       2      0      1    9.3500     0         1            1.0   \n",
       "4       1      1      1   26.2500     0         6            1.0   \n",
       "\n",
       "   ticket_hash_1  ticket_hash_2  ticket_hash_3  ...  ticket_hash_44  \\\n",
       "0            0.0            0.0            0.0  ...             0.0   \n",
       "1            0.0            0.0            0.0  ...             0.0   \n",
       "2            0.0            0.0            0.0  ...             0.0   \n",
       "3            0.0            0.0            0.0  ...             0.0   \n",
       "4            0.0            1.0           -2.0  ...             0.0   \n",
       "\n",
       "   ticket_hash_45  ticket_hash_46  ticket_hash_47  ticket_hash_48  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   ticket_hash_49  deck  embarked_C  embarked_Q  embarked_S  \n",
       "0             0.0    10           0           0           1  \n",
       "1             0.0    10           0           0           1  \n",
       "2             0.0     2           0           0           1  \n",
       "3             0.0    10           0           0           1  \n",
       "4             0.0    10           0           0           1  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "anticipated-brother",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nhashnum: 3\\nhashnum: 3, best score: 0.8244453899703341, best params: {'max_features': 0.5462302825379028, 'min_samples_split': 0.09669521448351819, 'n_estimators': 756}\\n\\nhashnum: 5\\nhashnum: 5, best score: 0.8398988287298041, best params: {'max_features': 0.5469872684267783, 'min_samples_split': 0.0030892523169961266, 'n_estimators': 543}\\n\\nhashnum: 7\\nhashnum: 7, best score: 0.834278859223014, best params: {'max_features': 0.5440545314599111, 'min_samples_split': 0.0183580831418924, 'n_estimators': 698}\\n\\nhashnum: 9\\nhashnum: 9, best score: 0.8356498717630512, best params: {'max_features': 0.5656116372467109, 'min_samples_split': 0.009708085754557828, 'n_estimators': 493}\\n\\nhashnum: 11\\nhashnum: 11, best score: 0.8370799796711933, best params: {'max_features': 0.5554708085387524, 'min_samples_split': 0.006184165978744427, 'n_estimators': 442}\\n\\nhashnum: 13\\nhashnum: 13, best score: 0.8398811001193726, best params: {'max_features': 0.5474618776191431, 'min_samples_split': 0.002879347385951991, 'n_estimators': 574}\\n\\nhashnum: 30\\nhashnum: 30, best score: 0.8384864494320935, best params: {'max_features': 0.5523470378125048, 'min_samples_split': 0.0181188798273324, 'n_estimators': 787}\\n\\n****************************\\nhashnum: 50\\nhashnum: 50, best score: 0.844100509402073, best params: {'max_features': 0.5629948810999708, 'min_samples_split': 0.00593040590826378, 'n_estimators': 469}\\n****************************\\n\\nhashnum: 100\\nhashnum: 100, best score: 0.842694039641173, best params: {'max_features': 0.555596535988239, 'min_samples_split': 0.00757142735563191, 'n_estimators': 608}\\n\\nhashnum: 200\\nhashnum: 200, best score: 0.8370799796711933, best params: {'max_features': 0.5580066272141194, 'min_samples_split': 0.004041476710163227, 'n_estimators': 640}\\n\\nhashnum: 500\\n\\nALRIGHT so then I guess let's try hashnum of 50\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "hashnum: 3\n",
    "hashnum: 3, best score: 0.8244453899703341, best params: {'max_features': 0.5462302825379028, 'min_samples_split': 0.09669521448351819, 'n_estimators': 756}\n",
    "\n",
    "hashnum: 5\n",
    "hashnum: 5, best score: 0.8398988287298041, best params: {'max_features': 0.5469872684267783, 'min_samples_split': 0.0030892523169961266, 'n_estimators': 543}\n",
    "\n",
    "hashnum: 7\n",
    "hashnum: 7, best score: 0.834278859223014, best params: {'max_features': 0.5440545314599111, 'min_samples_split': 0.0183580831418924, 'n_estimators': 698}\n",
    "\n",
    "hashnum: 9\n",
    "hashnum: 9, best score: 0.8356498717630512, best params: {'max_features': 0.5656116372467109, 'min_samples_split': 0.009708085754557828, 'n_estimators': 493}\n",
    "\n",
    "hashnum: 11\n",
    "hashnum: 11, best score: 0.8370799796711933, best params: {'max_features': 0.5554708085387524, 'min_samples_split': 0.006184165978744427, 'n_estimators': 442}\n",
    "\n",
    "hashnum: 13\n",
    "hashnum: 13, best score: 0.8398811001193726, best params: {'max_features': 0.5474618776191431, 'min_samples_split': 0.002879347385951991, 'n_estimators': 574}\n",
    "\n",
    "hashnum: 30\n",
    "hashnum: 30, best score: 0.8384864494320935, best params: {'max_features': 0.5523470378125048, 'min_samples_split': 0.0181188798273324, 'n_estimators': 787}\n",
    "\n",
    "****************************\n",
    "hashnum: 50\n",
    "hashnum: 50, best score: 0.844100509402073, best params: {'max_features': 0.5629948810999708, 'min_samples_split': 0.00593040590826378, 'n_estimators': 469}\n",
    "****************************\n",
    "\n",
    "hashnum: 100\n",
    "hashnum: 100, best score: 0.842694039641173, best params: {'max_features': 0.555596535988239, 'min_samples_split': 0.00757142735563191, 'n_estimators': 608}\n",
    "\n",
    "hashnum: 200\n",
    "hashnum: 200, best score: 0.8370799796711933, best params: {'max_features': 0.5580066272141194, 'min_samples_split': 0.004041476710163227, 'n_estimators': 640}\n",
    "\n",
    "hashnum: 500\n",
    "\n",
    "ALRIGHT so then I guess let's try hashnum of 50\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "accomplished-contest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 60 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Pclass          418 non-null    int64  \n",
      " 1   SibSp           418 non-null    int64  \n",
      " 2   Parch           418 non-null    int64  \n",
      " 3   Fare            418 non-null    float64\n",
      " 4   male            418 non-null    uint8  \n",
      " 5   age_bins        418 non-null    int32  \n",
      " 6   ticket_hash_0   418 non-null    float64\n",
      " 7   ticket_hash_1   418 non-null    float64\n",
      " 8   ticket_hash_2   418 non-null    float64\n",
      " 9   ticket_hash_3   418 non-null    float64\n",
      " 10  ticket_hash_4   418 non-null    float64\n",
      " 11  ticket_hash_5   418 non-null    float64\n",
      " 12  ticket_hash_6   418 non-null    float64\n",
      " 13  ticket_hash_7   418 non-null    float64\n",
      " 14  ticket_hash_8   418 non-null    float64\n",
      " 15  ticket_hash_9   418 non-null    float64\n",
      " 16  ticket_hash_10  418 non-null    float64\n",
      " 17  ticket_hash_11  418 non-null    float64\n",
      " 18  ticket_hash_12  418 non-null    float64\n",
      " 19  ticket_hash_13  418 non-null    float64\n",
      " 20  ticket_hash_14  418 non-null    float64\n",
      " 21  ticket_hash_15  418 non-null    float64\n",
      " 22  ticket_hash_16  418 non-null    float64\n",
      " 23  ticket_hash_17  418 non-null    float64\n",
      " 24  ticket_hash_18  418 non-null    float64\n",
      " 25  ticket_hash_19  418 non-null    float64\n",
      " 26  ticket_hash_20  418 non-null    float64\n",
      " 27  ticket_hash_21  418 non-null    float64\n",
      " 28  ticket_hash_22  418 non-null    float64\n",
      " 29  ticket_hash_23  418 non-null    float64\n",
      " 30  ticket_hash_24  418 non-null    float64\n",
      " 31  ticket_hash_25  418 non-null    float64\n",
      " 32  ticket_hash_26  418 non-null    float64\n",
      " 33  ticket_hash_27  418 non-null    float64\n",
      " 34  ticket_hash_28  418 non-null    float64\n",
      " 35  ticket_hash_29  418 non-null    float64\n",
      " 36  ticket_hash_30  418 non-null    float64\n",
      " 37  ticket_hash_31  418 non-null    float64\n",
      " 38  ticket_hash_32  418 non-null    float64\n",
      " 39  ticket_hash_33  418 non-null    float64\n",
      " 40  ticket_hash_34  418 non-null    float64\n",
      " 41  ticket_hash_35  418 non-null    float64\n",
      " 42  ticket_hash_36  418 non-null    float64\n",
      " 43  ticket_hash_37  418 non-null    float64\n",
      " 44  ticket_hash_38  418 non-null    float64\n",
      " 45  ticket_hash_39  418 non-null    float64\n",
      " 46  ticket_hash_40  418 non-null    float64\n",
      " 47  ticket_hash_41  418 non-null    float64\n",
      " 48  ticket_hash_42  418 non-null    float64\n",
      " 49  ticket_hash_43  418 non-null    float64\n",
      " 50  ticket_hash_44  418 non-null    float64\n",
      " 51  ticket_hash_45  418 non-null    float64\n",
      " 52  ticket_hash_46  418 non-null    float64\n",
      " 53  ticket_hash_47  418 non-null    float64\n",
      " 54  ticket_hash_48  418 non-null    float64\n",
      " 55  ticket_hash_49  418 non-null    float64\n",
      " 56  deck            418 non-null    int64  \n",
      " 57  embarked_C      418 non-null    uint8  \n",
      " 58  embarked_Q      418 non-null    uint8  \n",
      " 59  embarked_S      418 non-null    uint8  \n",
      "dtypes: float64(51), int32(1), int64(4), uint8(4)\n",
      "memory usage: 183.0 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nwhat a shitty to use API\\n\\nThe order of the columns in the transformed feature matrix follows the order of how the columns are specified in the\\ntransformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed\\nfeature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right\\nto the output of the transformers.\\n\\n\\n\\n# transformed columns have the passthrough columns added after, so need to come up with new column order and rename\\npassthrough_columns = [x for x in pipe.named_steps.init_clean.final_columns if x not in standardize_columns]\\n\\ntitanic_test_clean.columns = standardize_columns + passthrough_columns\\n\\ntitanic_test_clean['Fare'] = titanic_test_clean['Fare'].fillna(titanic_test_clean['Fare'].mean())\\n\\n#\\n\\ntest_results = pd.DataFrame(new_forest_clf.predict(titanic_test_clean).astype(int))\\n\\ntest_results = pd.concat([test_set_ids, test_results], axis=1)\\n\\ntest_results.columns = ['PassengerId', 'survived']\\n\\ntest_results.to_csv('titanic_test_results.csv', index=False)\\n\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_forest_clf2 = rscv.best_estimator_\n",
    "\n",
    "cleaner = InitAttributeCleaner2(ticket_handle='hash', hash_number=50)\n",
    "\n",
    "titanic_test_clean2 = pd.DataFrame(cleaner.fit_transform(titanic_test))\n",
    "\n",
    "print(titanic_test_clean2.info())\n",
    "\n",
    "test_results2 = pd.DataFrame(new_forest_clf2.predict(titanic_test_clean2).astype(int))\n",
    "\n",
    "test_results2 = pd.concat([test_set_ids, test_results2], axis=1)\n",
    "\n",
    "test_results2.columns = ['PassengerId', 'survived']\n",
    "\n",
    "test_results2.to_csv('titanic_test_results6.csv', index=False)\n",
    "'''\n",
    "what a shitty to use API\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the\n",
    "transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed\n",
    "feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right\n",
    "to the output of the transformers.\n",
    "\n",
    "\n",
    "\n",
    "# transformed columns have the passthrough columns added after, so need to come up with new column order and rename\n",
    "passthrough_columns = [x for x in pipe.named_steps.init_clean.final_columns if x not in standardize_columns]\n",
    "\n",
    "titanic_test_clean.columns = standardize_columns + passthrough_columns\n",
    "\n",
    "titanic_test_clean['Fare'] = titanic_test_clean['Fare'].fillna(titanic_test_clean['Fare'].mean())\n",
    "\n",
    "#\n",
    "\n",
    "test_results = pd.DataFrame(new_forest_clf.predict(titanic_test_clean).astype(int))\n",
    "\n",
    "test_results = pd.concat([test_set_ids, test_results], axis=1)\n",
    "\n",
    "test_results.columns = ['PassengerId', 'survived']\n",
    "\n",
    "test_results.to_csv('titanic_test_results.csv', index=False)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
